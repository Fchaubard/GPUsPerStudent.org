{
  "university_name": "Massachusetts Institute of Technology",
  "data_retrieved_date": "2025-12-14",
  "sources": [
    {
      "url": "https://orcd-docs.mit.edu/running-jobs/available-resources/",
      "data_found": "Engaging public partitions GPU node inventory (counts of nodes, GPUs/node, GPU models) including L40S (3- and 4-GPU nodes), H100 80GB (4 GPUs/node), H200 (8 GPUs/node), A100 80GB PCIe (4 GPUs/node), A100-SXM4-80GB (4 GPUs/node). Page dated Aug 5, 2025."
    },
    {
      "url": "https://orcd-docs.mit.edu/orcd-systems/",
      "data_found": "OpenMind: 340 GPUs total including 142 A100-80GB GPUs. (Also notes Engaging has 'over 1000 GPU cards including A100, RTX6000, H100, H200 and L40S', without a per-model breakdown.)"
    },
    {
      "url": "https://mit-supercloud.github.io/supercloud-docs/systems-and-software/",
      "data_found": "MGHPCC TX-E1: 224 nodes with 2\u00d7 NVIDIA Volta V100 32GB GPUs/node (Total GPUs 448)."
    },
    {
      "url": "https://mit-satori.github.io/satori-basics.html",
      "data_found": "Satori: 64 nodes, each with 4\u00d7 NVIDIA V100 32GB GPUs/node."
    },
    {
      "url": "https://orcd-docs.mit.edu/recipes/h100_getting_started/",
      "data_found": "Satori H100 nodes (IBM Watson AI Lab): as of 2023-06-19, 4 systems installed, each with 8\u00d7 H100 GPUs/system."
    },
    {
      "url": "https://www1.psfc.mit.edu/computers/cluster/gpu.html",
      "data_found": "PSFC GPU cluster: 6 nodes with 4 GPUs each; 3 nodes have 4\u00d7 V100 GPUs/node, and 3 nodes have 4\u00d7 RTX6000 GPUs/node."
    },
    {
      "url": "https://registrar.mit.edu/statistics-reports/enrollment-statistics-year/2024-2025",
      "data_found": "Student enrollment: 661 undergrad, 462 grad, 691 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 661,
    "grad_cs_count": 462,
    "phd_cs_count": 691,
    "year": "Fall 2024 (AY 2024-2025)",
    "source_url": "https://registrar.mit.edu/statistics-reports/enrollment-statistics-year/2024-2025",
    "notes": "Source is MIT Registrar page explicitly labeled \"Fall Term 2024-2025\". Undergrad CS count taken from the row \"Computer Science and Engineering, VI-3\" (Total UG = 661). Masters-only total (grad_cs_count) computed as Master/Eng enrollments from: \"Electrical Engineering and Computer Science, VI\" (152) + \"Electrical Engineering and Computer Science, VI-P (M.Eng.)\" (303) + \"Electrical Eng and Computer Science, VI-PA (M.Eng., Internship)\" (7) = 462. PhD-only total (phd_cs_count) computed from doctoral enrollments in \"Electrical Engineering and Computer Science, VI\": Doc Regular (688) + Doc Non-Res (3) = 691. Note: MIT reports graduate enrollment under the combined EECS department (Course VI), so the graduate counts are EECS (EE+CS combined) rather than CS-only. No explicit 'last updated' date was visible on the page."
  },
  "gpu_resources": {
    "h100_sxm_count": 32,
    "h100_pcie_count": 48,
    "h200_count": 96,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 210,
    "a100_40gb_count": 0,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 259,
    "v100_count": 24,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA RTX6000 (Quadro RTX 6000)",
        "count": 12,
        "notes": "PSFC GPU cluster: 3 nodes \u00d7 4 RTX6000/node = 12"
      }
    ],
    "source_url": "https://orcd-docs.mit.edu/running-jobs/available-resources/",
    "notes": "Cluster-by-cluster breakdown (nodes \u00d7 GPUs_per_node = total):\n\n1) Engaging (ORCD Docs 'Available Resources' page; unique physical nodes counted once even if listed in both mit_normal_gpu and mit_preemptable):\n- L40S: (node3502) 1 node \u00d7 3 = 3; plus 62 nodes \u00d7 4 = 248; plus (node2643-2644) 2 nodes \u00d7 4 = 8. L40S total = 3 + 248 + 8 = 259.\n- H100 80GB: node2906 1 \u00d7 4 = 4; plus node2640-2642 3 \u00d7 4 = 12; plus 8 nodes \u00d7 4 = 32. H100 total on Engaging = 4 + 12 + 32 = 48. (Form factor not specified; assumed PCIe because 4-GPU servers are commonly PCIe.)\n- H200: 12 nodes \u00d7 8 = 96.\n- A100 80GB PCIe: 4 nodes \u00d7 4 = 16.\n- A100-SXM4-80GB: 13 nodes \u00d7 4 = 52.\n=> A100 80GB total on Engaging = 16 + 52 = 68.\n\n2) OpenMind (ORCD Systems page):\n- A100-80GB GPUs = 142 (node count / GPUs-per-node not provided on the cited page).\n\n3) SuperCloud (TX-E1):\n- V100 32GB: 224 nodes \u00d7 2 = 448.\n\n4) Satori (base system):\n- V100 32GB: 64 nodes \u00d7 4 = 256.\n\n5) Satori (H100 add-on partition, cited as 2023-06-19):\n- H100: 4 systems \u00d7 8 = 32. (Assumed SXM/HGX-class because 8-GPU nodes.)\n\n6) PSFC GPU cluster:\n- V100: 3 nodes \u00d7 4 = 12.\n- RTX6000: 3 nodes \u00d7 4 = 12.\n\nTotals across all clusters above:\n- H100 SXM = 32; H100 PCIe = 48; H200 = 96; A100 80GB = 68 + 142 = 210; V100 = 448 + 256 + 12 = 716; L40S = 259.\n\nImportant limitation: ORCD Systems states Engaging has 'over 1000 GPU cards' including RTX6000, but public documentation used here only enumerates specific GPU nodes in Engaging public partitions; no authoritative per-model inventory was found for the remainder (e.g., RTX6000 counts on Engaging), so this JSON should be treated as a lower bound for some models."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches.",
  "validation_notes": "GPU CHANGES: Removed 448 V100 GPUs from MGHPCC TX-E1 because MGHPCC is a shared resource. Removed 256 V100 GPUs from Satori because Satori is an IBM Watson AI Lab resource. Only the PSFC GPU cluster V100s are counted."
}