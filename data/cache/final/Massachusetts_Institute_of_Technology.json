{
  "university_name": "Massachusetts Institute of Technology (MIT)",
  "data_retrieved_date": "2025-12-19",
  "sources": [
    {
      "url": "https://orcd-docs.mit.edu/running-jobs/overview/",
      "data_found": "Engaging Cluster (Main Student/Campus Cluster) partitions: mit_normal_gpu has 1 node (4x H100), 8 nodes (8x H200 = 64), 49 nodes (4x L40S = 196). These are the primary high-tier GPUs available to all students."
    },
    {
      "url": "https://orcd.mit.edu/news/orcd-newsletter-april-2024",
      "data_found": "ORCD reports Engaging cluster has 366 GPUs online. Mentions 32 H100 GPUs already active and 96 more H100s in the process of being added specifically for campus research."
    },
    {
      "url": "https://orcd.mit.edu/resources/mit-campus-wide-resources",
      "data_found": "OpenMind (Neuroscience) is being migrated to the Engaging cluster in 2024-2025, bringing 142 NVIDIA A100-80GB GPUs into the institute-wide accessible pool."
    },
    {
      "url": "https://mit-satori.github.io/satori-basics.html",
      "data_found": "Satori cluster: 64 nodes with 4x NVIDIA V100 32GB GPUs each (total 256). Open to all MIT students with Kerberos IDs."
    },
    {
      "url": "https://supercloud.mit.edu/",
      "data_found": "MIT SuperCloud (Open Research Partition): Provides access to approximately 850 NVIDIA Volta GPUs to the entire MIT community, including students for non-classified/open research."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 1699,
    "grad_cs_count": 363,
    "phd_cs_count": 850,
    "year": "2024-2025",
    "source_url": "https://dspace.mit.edu/bitstream/handle/1721.1/162054/EECS%20annual%20report%202025.pdf",
    "notes": "Data represents EECS enrollment. Undergrad count includes all CS-track majors (6-3, 6-4, etc.). Grad count covers MEng. PhD count covers Course 6 doctoral students."
  },
  "gpu_resources": {
    "h100_sxm_count": 128,
    "h100_pcie_count": 0,
    "h200_count": 64,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 142,
    "a100_40gb_count": 0,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 196,
    "v100_count": 1106,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "AMD Instinct MI50",
        "estimated_total": 160,
        "notes": "Available on Satori for specific workloads; directly accessible by student accounts."
      }
    ],
    "source_url": "https://orcd-docs.mit.edu/running-jobs/overview/",
    "notes": "FILTERED TOTALS (Directly Accessible to Students / Non-Shared Missions):\n\n1) Engaging (Campus-Wide): \n   - H200: 64 (8 nodes x 8 GPUs)\n   - H100: 32 (Existing) + 96 (2024 additions) = 128 total.\n   - L40S: 196 (49 nodes x 4 GPUs)\n\n2) OpenMind (Migrated to Campus-Wide): \n   - A100-80GB: 142. Note: Historically for Brain & Cog Sci but now part of the Engaging ORCD institute-wide pool.\n\n3) Satori (Open Access): \n   - V100: 256. Fully accessible to any student with Kerberos.\n\n4) SuperCloud (Open Research Partition): \n   - V100: 850. While Lincoln Lab manages the facility, the 'Open Research' partition is dedicated to campus users (students/faculty) and is separated from the Lab's mission-restricted hardware. \n\nEXCLUSIONS: TX-GAIA and TX-GAIN clusters are excluded as they are primarily for mission-driven work with restricted federal/DoD project overlap, even if some students gain incidental access via specific labs."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not specified for 2025."
  },
  "analysis_notes": "This analysis focuses strictly on hardware where an MIT student can log in using their Kerberos ID and submit jobs to a public queue (e.g., mit_normal_gpu) without being part of a specific DoD-funded project. The massive expansion of H200 and L40S resources in 2024-2025 marks a significant shift in campus-accessible compute."
}