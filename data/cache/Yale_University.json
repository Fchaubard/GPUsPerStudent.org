{
  "university_name": "Yale University",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://docs.ycrc.yale.edu/clusters/bouchet/",
      "data_found": "Bouchet overview states 80 NVIDIA H200 GPUs and 48 NVIDIA RTX 5000 Ada GPUs. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/bouchet/?utm_source=openai)); Bouchet public partition tables show H200 nodes have 8 GPUs/node; L40s nodes have 4 GPUs/node with count=10 in public partitions listing. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/bouchet/))"
    },
    {
      "url": "https://docs.ycrc.yale.edu/clusters/hopper/",
      "data_found": "Hopper gpu partition table: 15 nodes \u00d7 4 H100; 4 nodes \u00d7 8 H200; 9 nodes \u00d7 4 L40s; 10 nodes \u00d7 4 A40. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/hopper/?utm_source=openai))"
    },
    {
      "url": "https://docs.ycrc.yale.edu/clusters/misha/",
      "data_found": "Misha gpu partition table: 14 total H100 nodes (8+4+2) \u00d7 4; 4 nodes \u00d7 4 H200; 3 nodes \u00d7 4 L40s; 4 nodes \u00d7 4 A40; 6 nodes \u00d7 4 A100-80GB. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/misha/?utm_source=openai))"
    },
    {
      "url": "https://docs.ycrc.yale.edu/clusters/milgram/",
      "data_found": "Milgram gpu partition table: 3 nodes \u00d7 4 H100 (plus additional non-target GPUs such as RTX5000). ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/milgram/?utm_source=openai))"
    },
    {
      "url": "https://docs.ycrc.yale.edu/clusters/grace/",
      "data_found": "Grace gpu partition table includes: A100-80GB nodes (3 total) \u00d7 4; A100-40GB nodes (2 total) \u00d7 4; V100 nodes (4\u00d74 plus 2\u00d72). ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/grace/))"
    },
    {
      "url": "https://docs.ycrc.yale.edu/clusters/mccleary/",
      "data_found": "McCleary gpu partition table includes: A100-80GB nodes (3 total) \u00d7 4. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/mccleary/)); McCleary gpu_devel table includes: 1 node \u00d7 4 A100-40GB. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/mccleary/))"
    },
    {
      "url": "https://research.computing.yale.edu/posts/2025-08-26-ai-initiative-gpus-now-available",
      "data_found": "AI Initiative allocation summary: 80 H200 on Bouchet; 32 H200 on Hopper; 12 H100 on Milgram. (Used as consistency check for H200/H100 totals.) ([research.computing.yale.edu](https://research.computing.yale.edu/posts/2025-08-26-ai-initiative-gpus-now-available?utm_source=openai))"
    },
    {
      "url": "https://engineering.yale.edu/academic-study/departments/computer-science/undergraduate-study/career-fair",
      "data_found": "Student enrollment: 700 undergrad, 0 grad, 0 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 700,
    "grad_cs_count": 0,
    "phd_cs_count": 0,
    "year": "2024",
    "source_url": "https://engineering.yale.edu/academic-study/departments/computer-science/undergraduate-study/career-fair",
    "notes": "Source page explicitly references \u201cComputer Science Career Fair 2024\u201d. On the same page under \u201cAbout Yale Computer Science\u201d, it states: \u201cThe Department of Computer Science is home to over 850 students (~700 undergrad, ~150 MS/PhD)\u201d. I used ~700 as the undergrad CS count. The page does NOT provide a breakdown of the ~150 graduate students into MS-only vs PhD-only; therefore grad_cs_count and phd_cs_count are set to 0 (data not available at the required granularity from this 2024-labeled source)."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 128,
    "h200_count": 128,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 48,
    "a100_40gb_count": 12,
    "a40_count": 56,
    "a6000_count": 0,
    "l40s_count": 88,
    "v100_count": 20,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA RTX 5000 Ada",
        "vram_gb": 32,
        "count": 48,
        "clusters": [
          "Bouchet"
        ],
        "note": "Bouchet overview text states 48 RTX 5000 Ada GPUs; public gpu partition table shows 8 nodes \u00d7 4 = 32. Count here follows the stated total; see notes."
      }
    ],
    "source_url": "https://docs.ycrc.yale.edu/clusters/bouchet/",
    "notes": "Cluster-by-cluster math (nodes \u00d7 GPUs_per_node = total GPUs), using YCRC docs tables/cluster pages.\n\nBouchet:\n- H200: docs state 80 total; tables show 8 GPUs/node on H200 nodes => 10 nodes \u00d7 8 = 80 H200. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/bouchet/?utm_source=openai))\n- L40s: 10 nodes \u00d7 4 = 40 L40s. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/bouchet/))\n- RTX 5000 Ada (not in main counters): docs state 48 total; gpu partition table shows 8 nodes \u00d7 4 = 32; recorded as 48 in other_high_vram_gpus per stated total. ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/bouchet/?utm_source=openai))\n\nHopper:\n- H100: 15 \u00d7 4 = 60\n- H200: 4 \u00d7 8 = 32\n- L40s: 9 \u00d7 4 = 36\n- A40: 10 \u00d7 4 = 40 ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/hopper/?utm_source=openai))\n\nMisha:\n- H100: (8+4+2)=14 nodes \u00d7 4 = 56\n- H200: 4 \u00d7 4 = 16\n- L40s: 3 \u00d7 4 = 12\n- A40: 4 \u00d7 4 = 16\n- A100-80GB: 6 \u00d7 4 = 24 ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/misha/?utm_source=openai))\n\nMilgram:\n- H100: 3 \u00d7 4 = 12 ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/milgram/?utm_source=openai))\n\nGrace:\n- A100-80GB: (2+1)=3 nodes \u00d7 4 = 12\n- A100-40GB: 2 \u00d7 4 = 8\n- V100: (4 \u00d7 4) + (2 \u00d7 2) = 16 + 4 = 20 ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/grace/))\n\nMcCleary:\n- A100-80GB: (1+1+1)=3 nodes \u00d7 4 = 12 ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/mccleary/))\n- A100-40GB: 1 node \u00d7 4 = 4 (from gpu_devel table) ([docs.ycrc.yale.edu](https://docs.ycrc.yale.edu/clusters/mccleary/))\n\nTotals computed from above:\n- H100 total = 60 (Hopper) + 56 (Misha) + 12 (Milgram) = 128; counted as PCIe (docs do not specify SXM vs PCIe; 4 GPUs/node commonly indicates PCIe servers) => h100_pcie_count=128, h100_sxm_count=0.\n- H200 total = 80 (Bouchet) + 32 (Hopper) + 16 (Misha) = 128\n- A100-80GB total = 12 (Grace) + 12 (McCleary) + 24 (Misha) = 48\n- A100-40GB total = 8 (Grace) + 4 (McCleary) = 12\n- V100 total = 20 (Grace)\n- L40s total = 40 (Bouchet) + 36 (Hopper) + 12 (Misha) = 88\n- A40 total = 40 (Hopper) + 16 (Misha) = 56\n\nExclusions/assumptions:\n- Only counted GPUs explicitly shown in public partition hardware tables (and Bouchet\u2019s stated totals), not GPUs that may exist solely in PI/private partitions.\n- H100 SXM vs PCIe is not stated in the YCRC tables; classified all H100 as PCIe for counting purposes. [WARNING: Original source was inaccessible, using fallback.]"
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}