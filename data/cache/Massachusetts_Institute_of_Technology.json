{
  "university_name": "Massachusetts Institute of Technology",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://orcd-docs.mit.edu/running-jobs/available-resources/",
      "data_found": "Engaging public partitions GPU inventory: L40S (1 node x3; 62 nodes x4; plus 2 preemptable nodes x4), H100 80GB (1 node x4; 3 nodes x4; 8 nodes x4), H200 (12 nodes x8), A100 80GB PCIe (4 nodes x4), A100-SXM4-80GB (13 nodes x4)."
    },
    {
      "url": "https://orcd-docs.mit.edu/orcd-systems/",
      "data_found": "OpenMind: ~70 nodes, 340 GPUs total including 142 A100-80GB GPUs; Satori is an ORCD-supported system (details in Satori docs)."
    },
    {
      "url": "https://mit-satori.github.io/satori-basics.html",
      "data_found": "Satori: 64 nodes; each node has 4 NVIDIA V100 32GB GPUs."
    },
    {
      "url": "https://mit-supercloud.github.io/supercloud-docs/systems-and-software/",
      "data_found": "SuperCloud (MGHPCC TX-E1): 224 GPU nodes; each has 2 NVIDIA Volta V100 32GB GPUs; total GPUs 448."
    },
    {
      "url": "https://www1.psfc.mit.edu/computers/cluster/gpu.html",
      "data_found": "PSFC GPU cluster: 6 nodes x4 GPUs each; 3 nodes have 4x V100; 3 nodes have 4x RTX6000."
    },
    {
      "url": "https://registrar.mit.edu/statistics-reports/enrollment-statistics-year/2024-2025",
      "data_found": "Student enrollment: 661 undergrad, 462 grad, 691 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 661,
    "grad_cs_count": 462,
    "phd_cs_count": 691,
    "year": "Fall Term 2024-2025",
    "source_url": "https://registrar.mit.edu/statistics-reports/enrollment-statistics-year/2024-2025",
    "notes": "Source is MIT Registrar page titled \"Enrollment statistics by year 2024-2025\"; table header explicitly says \"Fall Term 2024-2025\". Undergraduate CS used row \"Computer Science and Engineering, VI-3\" with Total UG = 661. MIT Registrar does not break graduate enrollment down by VI-3; graduate CS is reported under EECS (Course VI) and related EECS masters programs. Masters-only count is Master/Eng. totals from: \"Electrical Engineering and Computer Science, VI\" = 152 plus \"Electrical Engineering and Computer Science, VI-P (M.Eng.)\" = 303 plus \"Electrical Eng and Computer Science, VI-PA (M.Eng., Internship)\" = 7, for 462 total masters/engineering students. PhD-only count is doctoral totals from \"Electrical Engineering and Computer Science, VI\": Doc Regular = 688 plus Doc Non-Res. = 3, for 691 total doctoral students. Page does not display a clear \"last updated\" date."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 48,
    "h200_count": 96,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 210,
    "a100_40gb_count": 0,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 259,
    "v100_count": 914,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA RTX 6000 (Quadro RTX 6000)",
        "count": 12,
        "cluster": "PSFC GPU Cluster",
        "vram_gb": 24
      }
    ],
    "source_url": "https://orcd-docs.mit.edu/orcd-systems/",
    "notes": "CLUSTERS COUNTED (nodes x GPUs_per_node = total):\n\n1) Engaging (ORCD Engaging Public Partitions; union of listed node sets, avoiding double-counting nodes repeated across partitions):\n- L40S: (62 nodes x4) + (1 node x3) + (2 nodes x4 additional preemptable-only) = 248 + 3 + 8 = 259 L40S\n- H100 80GB (assumed PCIe due to 4-GPU servers; interface not explicitly stated): (1 node x4) + (3 nodes x4) + (8 nodes x4) = 4 + 12 + 32 = 48 H100\n- H200: (12 nodes x8) = 96 H200\n- A100 80GB: (4 nodes x4 A100 80GB PCIe) + (13 nodes x4 A100-SXM4-80GB) = 16 + 52 = 68 A100 80GB\n\n2) OpenMind:\n- Total GPUs stated: 340 GPUs, including 142 A100-80GB.\n- Remaining 340 - 142 = 198 GPUs unspecified by model; per instruction, estimated as V100 (older than A100/H100 generation).\n=> OpenMind: 142 A100 80GB + 198 V100\n\n3) Satori:\n- (64 nodes x4 V100) = 256 V100\n\n4) SuperCloud (TX-E1 specs page):\n- (224 nodes x2 V100) = 448 V100\n\n5) PSFC GPU Cluster:\n- V100: (3 nodes x4) = 12 V100\n- RTX 6000: (3 nodes x4) = 12 RTX 6000\n\nTOTALS (summed across clusters above):\n- H100 PCIe: 48\n- H200: 96\n- A100 80GB: 68 + 142 = 210\n- V100: 448 + 256 + 12 + 198 = 914\n- L40S: 259\n\nNOTES/ASSUMPTIONS:\n- Engaging H100 interface (SXM vs PCIe) not stated on the Engaging public partitions page; categorized as PCIe based on typical 4-GPU server configurations (8-GPU configurations are more commonly SXM in DGX/HGX systems)."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}