{
  "university_name": "Stanford University",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://www.sherlock.stanford.edu/docs/tech/",
      "data_found": "Sherlock Tech Specs (as of Dec 2025): total GPUs=1,068; GPU partition has 32 GPU nodes broken down by type: 1x(4x P100 PCIe), 1x(4x P40), 2x(4x V100_SXM2), 1x(4x V100_SXM2), 2x(4x V100 PCIe), 16x(4x RTX_2080Ti), 2x(4x V100S PCIe), 4x(4x L40S), 2x(4x H100_SXM5), 1x(8x H100_SXM5)."
    },
    {
      "url": "https://cluster.cs.stanford.edu/tools/",
      "data_found": "SC Cluster 'Useful Tools' page includes example outputs for sphinx partition listing per-node GPU counts and totals: sphinx1-8 have 63 total A100 GPUs; sphinx9 has 8x H100; sphinx10-11 have 16 total H200; total GPUs in sphinx partition=87."
    },
    {
      "url": "https://irds.stanford.edu/data-findings/doctoral-enrollment-and-demographics",
      "data_found": "Student enrollment: 0 undergrad, 0 grad, 0 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 0,
    "grad_cs_count": 0,
    "phd_cs_count": 0,
    "year": "NO_RECENT_DATA_FOUND",
    "source_url": "https://irds.stanford.edu/data-findings/doctoral-enrollment-and-demographics",
    "notes": "Searched for Stanford Computer Science (CS) student enrollment counts specifically for Fall 2024 / AY 2024-2025 using the requested strategy. Official Stanford Facts pages confirm overall Fall 2024 enrollment totals (not CS-specific): undergraduates 7,554 and graduate students 9,915, but do not provide CS-major headcounts. The IR&DS Enrollment page provides a dashboard for enrollment by school/degree level and links to downloadable data via Google Drive, but CS-major/program counts were not publicly retrievable in the accessible pages. The IR&DS Doctoral Enrollment & Demographics page links to a Google Sheet (Doctoral_Enrollment.xlsx) that includes Academic Year values 2024 and 2025, but the accessible HTML view did not expose rows for the Computer Science PhD program (and exporting the full dataset was not accessible via the available links). Stanford CS department pages reviewed did not publish current student headcounts by degree level. Because no public Stanford source explicitly providing CS undergrad, CS master's-only, and CS PhD counts for Fall 2024 / AY 2024-25 was found, counts are set to 0 per instructions."
  },
  "gpu_resources": {
    "h100_sxm_count": 144,
    "h100_pcie_count": 0,
    "h200_count": 32,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 543,
    "a100_40gb_count": 160,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 16,
    "v100_count": 228,
    "p100_count": 4,
    "gh200_count": 0,
    "other_high_vram_gpus": [],
    "source_url": "https://www.sherlock.stanford.edu/docs/tech/",
    "notes": "Clusters counted: (1) Stanford SRCC Sherlock, (2) Stanford CS SC Cluster (sphinx partition as documented), (3) Stanford CS HAI Compute Cluster.\n\n(1) Sherlock (https://www.sherlock.stanford.edu/docs/tech/): total GPUs=1,068.\n  - Public `gpu` partition breakdown (nodes x GPUs/node):\n    * 1 x 4x Tesla P100 PCIe = 4 P100\n    * 1 x 4x Tesla P40 = 4 (not mapped to requested fields)\n    * 2 x 4x Tesla V100_SXM2 = 8 V100\n    * 1 x 4x Tesla V100_SXM2 = 4 V100\n    * 2 x 4x Tesla V100 PCIe = 8 V100\n    * 2 x 4x Tesla V100S PCIe = 8 V100 (counted under v100_count)\n    * 4 x 4x Tesla L40S = 16 L40S\n    * 2 x 4x Tesla H100_SXM5 = 8 H100 SXM\n    * 1 x 8x Tesla H100_SXM5 = 8 H100 SXM\n    * 16 x 4x RTX_2080Ti = 64 (not mapped to requested fields)\n    => Public `gpu` partition totals in requested fields: P100=4, V100=28, L40S=16, H100_SXM=16.\n    => Public `gpu` partition GPUs overall (all models) = 132.\n  - Sherlock `owners` partition GPUs are not broken down by model on the public Tech Specs page.\n    Remaining GPUs to attribute = 1,068 - 132 = 936 (MODEL UNKNOWN).\n    Per user instruction, estimated by cluster age/generation mix (newer=A100/H100/H200; older=V100):\n      * Estimated A100 80GB = 480\n      * Estimated A100 40GB = 160\n      * Estimated V100 = 200\n      * Estimated H100 SXM = 80\n      * Estimated H200 = 16\n      * Estimated H100 PCIe = 0\n    (These estimates sum to 936.)\n\n(2) SC Cluster (sphinx partition only, from https://cluster.cs.stanford.edu/tools/):\n  Node-level GPUs (implicit from GRES):\n    * sphinx1: 8x a100\n    * sphinx2: 7x a100\n    * sphinx3: 8x a100\n    * sphinx4: 8x a100\n    * sphinx5: 8x a100\n    * sphinx6: 8x a100\n    * sphinx7: 8x a100\n    * sphinx8: 8x a100\n    * sphinx9: 8x h100\n    * sphinx10: 8x h200\n    * sphinx11: 8x h200\n  Totals: A100=63, H100=8, H200=16. (Assumed A100 are 80GB; H100 assumed SXM-class since 8-GPU node; memory/form factor not explicitly stated on the page.)\n\n(3) HAI Compute Cluster (https://www4.cs.stanford.edu/haic):\n  Stated directly: 40x H100 GPUs with NVLink (counted as H100 SXM; PCIe vs SXM not explicitly stated).\n\nGrand totals (requested fields) = Sherlock(estimated owners + explicit public gpu partition) + SC(sphinx) + HAI:\n  - H100_SXM = (Sherlock 16 + est 80) + SC 8 + HAI 40 = 144\n  - H200 = (Sherlock est 16) + SC 16 = 32\n  - A100_80GB = (Sherlock est 480) + SC 63 = 543\n  - A100_40GB = Sherlock est 160\n  - V100 = (Sherlock 28 + est 200) = 228\n  - P100 = Sherlock 4\n  - L40S = Sherlock 16\n\nNote: GPUs present but not mapped to requested fields include Sherlock Tesla P40 (4) and RTX 2080 Ti (64), totaling 68 GPUs; this is why the sum of requested categories (1,127) is less than total GPUs across the three clusters counted (Sherlock 1,068 + SC sphinx 87 + HAI 40 = 1,195)."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}