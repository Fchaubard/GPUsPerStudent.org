{
  "university_name": "Harvard University",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://kempnerinstitute.harvard.edu/compute/",
      "data_found": "Kempner AI cluster: 384 Nvidia H100 80GB GPUs and 144 Nvidia A100 40GB GPUs."
    },
    {
      "url": "https://docs.rc.fas.harvard.edu/kb/kempner-partitions/",
      "data_found": "Kempner partitions: kempner_h100 has 384 H100 GPUs at 4 GPUs/node; kempner has 132 A100 GPUs at 4 GPUs/node; kempner_interactive has 24 A100 MIG instances at 8 MIG instances/node."
    },
    {
      "url": "https://docs.rc.fas.harvard.edu/kb/running-jobs/",
      "data_found": "FASRC Cannon public partitions: gpu is 36 nodes with 4 A100/node; gpu_h200 is 22 nodes with 4 H200/node; gpu_test is 12 nodes with 4 A100/node in MIG mode (2x 3g.20GB MIG instances per A100)."
    },
    {
      "url": "https://www.rc.fas.harvard.edu/blog/cannon-2-0/",
      "data_found": "Cannon 2.0: 36 GPU nodes with four A100 80GB GPUs per node (144 total). (Used to classify Cannon 'gpu' A100s as 80GB.)"
    },
    {
      "url": "https://it.hms.harvard.edu/service/longwood-hpc-cluster",
      "data_found": "HMS Longwood HPC: 8 DGX nodes each with 8 GPUs (64 H100 80GB total) + 2 Grace Hopper nodes each with 1 GPU (~96GB VRAM)."
    },
    {
      "url": "https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1629290761/Using%20O2%20GPU%20resources",
      "data_found": "HMS O2: 240 GPU cards total; includes V100 (8x16GB + 24x32GB), L40S (36 + 52 + 36), A100 80GB (4 + 22), A100 40GB (2) + A100 MIG cards 40GB (8), A40 (8), RTX 8000 (44), L40 (2)."
    },
    {
      "url": "https://www.nvidia.com/en-us/data-center/hgx/",
      "data_found": "NVIDIA HGX H100 4-GPU and 8-GPU platforms are specified as H100 SXM form factor (used to classify multi-GPU node H100s as SXM rather than PCIe)."
    },
    {
      "url": "https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/",
      "data_found": "NVIDIA GH200 Grace Hopper Superchip documentation (used to interpret 'Grace Hopper nodes' as GH200-class nodes)."
    },
    {
      "url": "https://seas.harvard.edu/prospective-students/prospective-graduate-students/graduate-student-data",
      "data_found": "Student enrollment: 177 undergrad, 0 grad, 150 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 177,
    "grad_cs_count": 0,
    "phd_cs_count": 150,
    "year": "Fall 2024",
    "source_url": "https://seas.harvard.edu/prospective-students/prospective-graduate-students/graduate-student-data",
    "notes": "Fall 2024 data only (no 2023-or-earlier sources used). PhD: SEAS 'Graduate Student Data' page explicitly states 'Graduate Enrollment (Fall 2024)' and lists 'Computer Science: 150 Ph.D.s' (so phd_cs_count=150). The same page lists masters by degree area but does NOT list a Computer Science masters line; therefore grad_cs_count (MS/Masters in CS only) is set to 0. Undergrad: SEAS news story dated November 27, 2024 ('A place for everyone') reports the 'Class of 2027 recently declared students include 177 in computer science' at Convocation (this is the number of newly declared second-year CS concentrators in that cohort, not total CS concentrators across all class years). Undergrad source page: https://seas.harvard.edu/news/2024/11/place-everyone"
  },
  "gpu_resources": {
    "h100_sxm_count": 448,
    "h100_pcie_count": 0,
    "h200_count": 96,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 170,
    "a100_40gb_count": 210,
    "a40_count": 8,
    "a6000_count": 0,
    "l40s_count": 124,
    "v100_count": 32,
    "p100_count": 0,
    "gh200_count": 2,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA Quadro RTX 8000 (48GB)",
        "count": 44
      },
      {
        "model": "NVIDIA L40 (48GB)",
        "count": 2
      }
    ],
    "source_url": "https://kempnerinstitute.harvard.edu/compute/",
    "notes": "CLUSTERS/ENVIRONMENTS COUNTED\n\n1) Kempner Institute AI cluster\n- H100: kempner_h100 partition states 384 H100 GPUs with 4 GPUs/node => 96 nodes \u00d7 4 = 384.\n- A100: kempner partition states 132 A100 GPUs with 4 GPUs/node => 33 nodes \u00d7 4 = 132.\n- Interactive A100 MIG: kempner_interactive states 24 MIG instances at 8 MIG instances/node => 3 nodes. Using the Cannon gpu_test MIG description (each A100 has two 3g.20GB MIG instances) implies 4 physical A100 GPUs/node => 3 nodes \u00d7 4 = 12 A100.\n- Total Kempner A100 physical GPUs = 132 + 12 = 144, matching Kempner compute page.\n- Counted Kempner H100s as SXM (multi-GPU HGX-style nodes).\n\n2) FASRC Cannon (public partitions)\n- gpu partition: 36 nodes \u00d7 4 A100/node = 144 A100. Classified as A100 80GB per Cannon 2.0 post.\n- gpu_test partition: 12 nodes \u00d7 4 A100/node = 48 A100 in MIG mode; classified as A100 40GB because MIG instances are 3g.20GB (2 instances per GPU).\n- gpu_h200 partition: 22 nodes \u00d7 4 H200/node = 88 H200.\n\n3) FASSE (FAS Secure Environment)\n- fasse_gpu: 2 nodes \u00d7 4 A100/node = 8 A100 (assumed 40GB based on earlier Cannon 2.0 partition labeling for fasse_gpu as A100 40GB).\n- fasse_gpu_h200: 2 nodes \u00d7 4 H200/node = 8 H200.\n- NOTE: This cleanly explains the discrepancy between '24 H200 nodes' originally announced vs current Cannon gpu_h200 showing 22 nodes (2 nodes appear assigned to FASSE as fasse_gpu_h200).\n\n4) HMS Longwood HPC cluster\n- H100: 8 DGX nodes \u00d7 8 GPUs/node = 64 H100 (assumed SXM because DGX/HGX multi-GPU nodes use SXM form factor).\n- Grace Hopper: 2 nodes \u00d7 1 GPU/node = 2 GH200.\n\n5) HMS O2 cluster (cards listed directly; nodes not provided on the page)\n- V100: 8 + 24 = 32\n- L40S: 36 + 52 + 36 = 124\n- A100 80GB: 4 + 22 = 26\n- A100 40GB: 2 + 8 (A100 MIG 'cards') = 10\n- A40: 8\n- Other: RTX 8000 (44), L40 (2)\n\nTOTALS\n- H100 SXM = 384 (Kempner) + 64 (Longwood DGX) = 448\n- H200 = 88 (Cannon gpu_h200) + 8 (FASSE_gpu_h200) = 96\n- A100 80GB = 144 (Cannon gpu) + 26 (O2) = 170\n- A100 40GB = 144 (Kempner) + 48 (Cannon gpu_test) + 8 (FASSE_gpu) + 10 (O2) = 210\n- V100 = 32 (O2 only)\n\nNOT FOUND/NOT COUNTED\n- Could not include Harvard University IT 'Central GPU Cluster' specs because the public page returned an HTTP 403 when fetched."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}