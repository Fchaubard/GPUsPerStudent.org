{
  "university_name": "University of Pittsburgh",
  "data_retrieved_date": "2025-12-14",
  "sources": [
    {
      "url": "https://crc.pitt.edu/overview-crc-services/computing-hardware",
      "data_found": "CRCD GPU cluster partitions: l40s (20 nodes, 4x L40S/node), a100 (10 nodes + 2 nodes, 4x A100 40GB PCIe/node), a100_multi (10 nodes, 4x A100 40GB PCIe/node), a100_nvlink (2 nodes, 8x A100 80GB SXM/node; plus 3 nodes, 8x A100 40GB SXM/node)."
    },
    {
      "url": "https://crc-pages.pitt.edu/user-manual/policies/resource-descriptions-for-writing-proposals/",
      "data_found": "CRCD summary statement: '32 GPU dedicated nodes with a total of 20 V100 GPUs and 128 A100 GPUs' (used as confirmation/source for V100 presence and count)."
    },
    {
      "url": "https://bits.csb.pitt.edu/training/",
      "data_found": "Department (CSB) cluster GPU inventory totals include: V100 (16GB): 4; A100 (40GB): 4; L40 (48GB): 32 (plus other smaller GPUs not tallied in requested fields)."
    },
    {
      "url": "https://www.crc.pitt.edu/become-h2p-owner",
      "data_found": "FY2026 GPU expansion plans mention H200 SXM and RTX PRO 6000 Blackwell Server Edition with 'anticipated deployment: November 2025' (plans only; not counted without installed-spec confirmation)."
    },
    {
      "url": "https://www.ir.pitt.edu/sites/default/files/assets/2024-2025_CDS_Pittsburgh_0.pdf",
      "data_found": "Student enrollment: 0 undergrad, 0 grad, 0 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 0,
    "grad_cs_count": 0,
    "phd_cs_count": 0,
    "year": "NO_RECENT_DATA_FOUND",
    "source_url": "https://www.ir.pitt.edu/sites/default/files/assets/2024-2025_CDS_Pittsburgh_0.pdf",
    "notes": "Looked specifically for University of Pittsburgh Computer Science (CS) student enrollment counts for AY 2024-2025 / Fall 2024. The official University of Pittsburgh Common Data Set (CDS) for AY 2024-2025 provides overall Pittsburgh Campus enrollment as of the official fall reporting date / Oct 15, 2024, but does NOT provide enrollment by major/program (e.g., CS majors). ([ir.pitt.edu](https://www.ir.pitt.edu/sites/default/files/assets/2024-2025_CDS_Pittsburgh_0.pdf)) In the same CDS, Section J reports disciplinary areas of DEGREES CONFERRED (July 1, 2023\u2013June 30, 2024), which is not the requested CS student enrollment headcount. ([ir.pitt.edu](https://www.ir.pitt.edu/sites/default/files/assets/2024-2025_CDS_Pittsburgh_0.pdf)) Also checked SCI's public 'At a Glance' page (it provides total SCI undergrad and total SCI graduate counts, but not CS-only counts), so it cannot be used to fill CS enrollment fields. ([sci.pitt.edu](https://www.sci.pitt.edu/about/sci-glance?utm_source=openai)) No official Pitt source with CS-only enrollment (separating undergrad vs MS vs PhD) for Fall 2024 / AY 2024-2025 was located in an accessible format."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 0,
    "h200_count": 0,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 16,
    "a100_40gb_count": 116,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 88,
    "v100_count": 24,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "L40 48GB",
        "count": 32,
        "source": "https://bits.csb.pitt.edu/training/"
      }
    ],
    "source_url": "https://crc.pitt.edu/overview-crc-services/computing-hardware",
    "notes": "CRCD GPU cluster (crc.pitt.edu Computing Hardware):\n- L40S: 20 nodes \u00d7 4 = 80\n- A100 40GB PCIe (a100): (10+2) nodes \u00d7 4 = 48\n- A100 40GB PCIe (a100_multi): 10 nodes \u00d7 4 = 40\n- A100 80GB SXM (a100_nvlink): 2 nodes \u00d7 8 = 16\n- A100 40GB SXM (a100_nvlink): 3 nodes \u00d7 8 = 24\n=> From CRCD GPU cluster: L40S=80; A100_40GB total=48+40+24=112; A100_80GB=16\n\nCRCD SRE GPU partition (access_sre):\n- L40S: 2 nodes \u00d7 4 = 8\n=> Total L40S (CRCD GPU cluster + SRE) = 80 + 8 = 88\n\nV100 (CRCD): resource-descriptions page states total V100 GPUs = 20 (node breakdown not provided on current hardware tables).\n\nCSB department cluster (bits.csb.pitt.edu): provides GPU totals (not node counts), so added directly:\n- V100 += 4 (now 20+4=24)\n- A100 40GB += 4 (now 112+4=116)\n- L40 48GB = 32 recorded under other_high_vram_gpus (not L40S).\n\nFinal totals: L40S=88; A100_40GB=116; A100_80GB=16; V100=24; all H100/H200/P100/A6000/L40S-variants not listed above assumed 0 based on current published specs."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}