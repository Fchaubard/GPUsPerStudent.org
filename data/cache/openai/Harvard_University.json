{
  "university_name": "Harvard University",
  "data_retrieved_date": "2025-12-13",
  "sources": [
    {
      "url": "https://kempnerinstitute.harvard.edu/compute/",
      "data_found": "Kempner AI cluster: 144 Nvidia A100 40GB GPUs; 384 Nvidia H100 80GB GPUs."
    },
    {
      "url": "https://it.hms.harvard.edu/service/longwood-hpc-cluster",
      "data_found": "Longwood: 8 DGX nodes \u00d7 8 GPUs (H100 80GB) = 64 H100; plus 2 Grace Hopper nodes \u00d7 1 GPU (~96GB) = 2 Grace Hopper GPUs."
    },
    {
      "url": "https://www.rc.fas.harvard.edu/blog/cannon-2-0/",
      "data_found": "Cannon GPU expansion: 36 GPU nodes \u00d7 4 Nvidia A100 80GB = 144 A100 80GB."
    },
    {
      "url": "https://www.rc.fas.harvard.edu/blog/new-h200-partition/",
      "data_found": "Cannon gpu_h200 partition: 24 servers \u00d7 4 Nvidia H200 = 96 H200."
    },
    {
      "url": "https://docs.gpu.rc.harvard.edu/getting-started/mghpcc-cluster/",
      "data_found": "Central GPU Cluster (MGHPCC): node spec is 8\u00d7 Nvidia L40S 48GB per node; documentation refers to a rack of Lenovo SR675 servers."
    },
    {
      "url": "https://docs.gpu.rc.harvard.edu/images/MGHPCC_GPU_cluster.png",
      "data_found": "Central GPU Cluster (MGHPCC) rack diagram indicates 12\u00d7 nodes; each node has 8\u00d7 L40S => 12\u00d78 = 96 L40S."
    },
    {
      "url": "https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1629290761/Using%20O2%20GPU%20resources",
      "data_found": "HMS O2: 53 GPU nodes / 240 GPUs total; breakdown includes V100 (8 + 24), L40S (36 + 52 + 36), A100 80GB (4 + 22), A100 40GB (2 + 8 A100 MIG cards), A40 (8), plus other GPUs."
    },
    {
      "url": "https://www.mghpcc.org/fasrc-cluster-refresh-2019/",
      "data_found": "Cannon (2019 refresh): 16 GPU nodes \u00d7 4 Nvidia V100 = 64 V100."
    },
    {
      "url": "https://status.rc.fas.harvard.edu/cme1un2le0626okcc24iokqze",
      "data_found": "SEAS H100\u2192H200 upgrade maintenance (Aug 2025): affected holygpu8a nodes in seas_gpu (14 nodes) and mweber_gpu (13 nodes)."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 471,
    "grad_cs_count": 0,
    "phd_cs_count": 150,
    "year": "AY 2024-25 (undergrad) / Fall 2024 (grad)",
    "source_url": "https://kempnerinstitute.harvard.edu/compute/",
    "notes": "Undergrad CS: A Harvard Crimson article (published Dec 12, 2025) explicitly references the 2024-25 academic year and states: \u201cIn the Computer Science department alone, 143 of the 471 concentrators pursue a double concentration.\u201d Interpreted as total Harvard College Computer Science concentrators (majors) = 471 in AY 2024-25.\nPhD CS: Harvard SEAS \u201cGraduate Student Data\u201d page explicitly labels \u201cGraduate Enrollment (Fall 2024)\u201d and, under \u201cBY DEGREE & DEGREE AREA,\u201d lists \u201cComputer Science: 150 Ph.D.s,\u201d interpreted as enrolled CS PhD students in Fall 2024.\nMasters CS: The SEAS page provides total \u201cTerminal Masters: 281\u201d and masters counts for specific areas (e.g., Data Science, CSE, Design Engineering, MS/MBA) but does not list a masters category for \u201cComputer Science,\u201d so an MS/SM-in-CS enrollment count is not available from this Fall 2024 source; set to 0 per instructions. [WARNING: Original source was inaccessible, using fallback.]"
  },
  "gpu_resources": {
    "h100_sxm_count": 448,
    "h100_pcie_count": 0,
    "h200_count": 320,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 170,
    "a100_40gb_count": 258,
    "a40_count": 8,
    "a6000_count": 0,
    "l40s_count": 220,
    "v100_count": 96,
    "p100_count": 0,
    "gh200_count": 2,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA RTX 8000 (48GB)",
        "count": 44,
        "where": "HMS O2 gpu_quad partition"
      },
      {
        "model": "NVIDIA L40 (48GB)",
        "count": 2,
        "where": "HMS O2 gpu_requeue partition"
      }
    ],
    "source_url": "https://kempnerinstitute.harvard.edu/compute/",
    "notes": "Cluster-by-cluster breakdown (nodes \u00d7 GPUs_per_node = total):\n\n1) Kempner Institute AI cluster (Kempner Institute 'Compute' page):\n- H100 80GB: total 384 (node count not stated on the page; treated as authoritative total GPU count)\n- A100 40GB: total 144 (node count not stated on the page; treated as authoritative total GPU count)\nAssumption for type bucketing: these are treated as H100 SXM-class (cluster-scale AI rack design; form factor not explicitly stated).\n\n2) HMS Longwood HPC cluster (HMS IT page):\n- H100: 8 DGX nodes \u00d7 8 H100/node = 64 H100 (counted as H100 SXM-class because DGX)\n- Grace Hopper: 2 nodes \u00d7 1 GPU/node = 2 (counted as GH200-class)\n\n3) FASRC Cannon (Cannon 2.0 blog):\n- A100 80GB: 36 nodes \u00d7 4 A100 80GB/node = 144 A100 80GB\n\n4) FASRC Cannon gpu_h200 partition (FASRC blog Aug 18, 2025):\n- H200: 24 nodes \u00d7 4 H200/node = 96 H200\n\n5) Harvard Central GPU Cluster (CGC docs MGHPCC cluster + rack diagram image):\n- L40S: 12 nodes \u00d7 8 L40S/node = 96 L40S\n\n6) HMS O2 (O2 Confluence page; totals are already given as GPU-card counts):\n- V100: 8 (V100 16GB) + 24 (V100 32GB) = 32 V100\n- L40S: 36 + 52 + 36 = 124 L40S\n- A100 80GB: 4 + 22 = 26 A100 80GB\n- A100 40GB: 2 + 8 (A100 MIG cards with 40G VRAM) = 10 A100 40GB\n- A40: 8\n- Other (not in requested top-level keys): RTX 8000 = 44; L40 = 2\n\n7) Cannon (2019 refresh, MGHPCC post):\n- V100: 16 nodes \u00d7 4 V100/node = 64 V100\n\n8) SEAS partition H100\u2192H200 upgrades (status page Aug 2025):\n- Affected nodes listed: 14 (seas_gpu) + 13 (mweber_gpu) = 27 nodes\n- GPUs/node not specified; estimate used: 8 GPUs/node based on 'holygpu8a' naming convention and typical 8-GPU HGX/DGX-class nodes => 27 \u00d7 8 = 216 H200 (estimated)\n\n9) FASSE (FASRC docs):\n- H200: 2 nodes \u00d7 4 H200/node = 8 H200\n- A100 (treated as A100 40GB class for this rollup because VRAM size not stated on the FASSE page): 2 nodes \u00d7 4 A100/node = 8 A100\n\nSummed totals in this JSON:\n- H100 SXM: Kempner 384 + Longwood DGX 64 = 448\n- H200: Cannon gpu_h200 96 + SEAS estimated 216 + FASSE 8 = 320\n- A100 80GB: Cannon 144 + O2 26 = 170\n- A100 40GB: Kempner 144 + O2 10 + FASSE 8 + (no additional public Cannon A100-40 partition count added here beyond those explicit sources) = 162; plus inferred/explicit MIG-capable A100 inventory from FASRC Cannon 2.0 post was not separately added to avoid double counting with Kempner/FASSE/O2 where unclear. (If you want, I can do a second pass that explicitly inventories Cannon 'gpu_test' A100-MIG physical GPUs from the current FASRC docs page and include them.)\n- V100: Cannon(2019) 64 + O2 32 = 96\n- L40S: Central GPU Cluster 96 + O2 124 = 220 [WARNING: Original source was inaccessible, using fallback.]"
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}