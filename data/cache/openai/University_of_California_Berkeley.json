{
  "university_name": "University of California, Berkeley",
  "data_retrieved_date": "2025-06-03",
  "sources": [
    {
      "url": "https://docs-research-it.berkeley.edu/services/high-performance-computing/user-guide/hardware-config/",
      "data_found": "Savio Cluster Hardware Configuration: Lists node counts and GPU specifications for Savio 2, Savio 3, and Savio 4 partitions. Savio 4 GPU: 6 nodes w/ 4x A100 80GB. Savio 3 GPU: 2 nodes w/ 4x A40, 3 nodes w/ 2x V100, 32 nodes w/ 4x GTX 2080Ti, 9 nodes w/ 4x Titan V. Savio 2 GPU: 13 nodes w/ 2x V100, 15 nodes w/ 2x V100, 116 nodes w/ 4x K80. Savio 2 1080ti: 16 nodes w/ 4x GTX 1080Ti."
    },
    {
      "url": "https://eecs.berkeley.edu/about/by-the-numbers",
      "data_found": "EECS by the Numbers (Fall 2023): Undergraduate Students: 3,924 (2,423 L&S CS + 1,501 CoE EECS). Graduate Students: 853 (349 Master's + 504 PhD)."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 3924,
    "grad_cs_count": 349,
    "phd_cs_count": 504,
    "year": "2023-2024",
    "source_url": "https://eecs.berkeley.edu/about/by-the-numbers",
    "notes": "Data represents Fall 2023 enrollment. 'Undergrad' combines Computer Science (L&S) and EECS (College of Engineering) majors. 'Grad' represents Master's students (MS/MEng) and 'PhD' represents Doctoral students as explicitly broken down on the department facts page."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 0,
    "h200_count": 0,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 24,
    "a100_40gb_count": 0,
    "a40_count": 8,
    "a6000_count": 0,
    "l40s_count": 0,
    "v100_count": 62,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA Tesla K80 (24GB)",
        "estimated_total": 464,
        "notes": "Savio 2 partition: 116 nodes \u00d7 4 GPUs. (Legacy hardware)."
      },
      {
        "model": "NVIDIA GTX 2080Ti (11GB)",
        "estimated_total": 128,
        "notes": "Savio 3 partition: 32 nodes \u00d7 4 GPUs."
      },
      {
        "model": "NVIDIA GTX 1080Ti (11GB)",
        "estimated_total": 64,
        "notes": "Savio 2 1080ti partition: 16 nodes \u00d7 4 GPUs."
      },
      {
        "model": "NVIDIA Titan V (12GB)",
        "estimated_total": 36,
        "notes": "Savio 3 partition: 9 nodes \u00d7 4 GPUs."
      }
    ],
    "source_url": "https://docs-research-it.berkeley.edu/services/high-performance-computing/user-guide/hardware-config/",
    "notes": "Counts reflect the central campus 'Savio' HPC cluster managed by Berkeley Research IT. \n\nCALCULATIONS:\n- A100 80GB: 6 nodes (Savio 4) \u00d7 4 GPUs = 24.\n- A40: 2 nodes (Savio 3) \u00d7 4 GPUs = 8.\n- V100: 3 nodes (Savio 3) \u00d7 2 GPUs + 13 nodes (Savio 2 Broadwell) \u00d7 2 GPUs + 15 nodes (Savio 2 Skylake) \u00d7 2 GPUs = 6 + 26 + 30 = 62.\n- K80: 116 nodes (Savio 2) \u00d7 4 GPUs = 464.\n- 2080Ti: 32 nodes (Savio 3) \u00d7 4 GPUs = 128.\n- 1080Ti: 16 nodes (Savio 2) \u00d7 4 GPUs = 64.\n- Titan V: 9 nodes (Savio 3) \u00d7 4 GPUs = 36.\n\nNOTE ON MISSING DATA: The Berkeley AI Research (BAIR) lab maintains a separate, private 'BAIR Commons' cluster (often cited as having significant A100/H100 resources), but a verifiable public inventory of node counts is not available. Additionally, Berkeley students frequently utilize NERSC (Perlmutter) at LBNL, but these are DOE resources, not university-owned, and are excluded from this count."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Berkeley's public hardware list (Savio) is heavily weighted toward older generations (K80/V100/Consumer cards) compared to peer institutions. This is likely because top-tier AI research at Berkeley occurs on the opaque 'BAIR Commons' cluster or the nearby NERSC Perlmutter supercomputer (LBNL), neither of which publish open hardware manifests for the university audit."
}