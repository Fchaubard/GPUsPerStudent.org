{
  "university_name": "Johns Hopkins University",
  "data_retrieved_date": "2025-12-13",
  "sources": [
    {
      "url": "https://docs.arch.jhu.edu/en/latest/1_Clusters/Rockfish/1_Resources/Hardware.html",
      "data_found": "Rockfish GPU nodes: 18 nodes with 4\u00d7A100 40GB; 10 nodes with 4\u00d7A100 80GB; 4 nodes with 8\u00d7L40S 48GB."
    },
    {
      "url": "https://docs.arch.jhu.edu/en/latest/1_Clusters/DSAI/1_Resources/Hardware.html",
      "data_found": "DSAI GPU nodes: 15 nodes with 8\u00d7A100 80GB; 16 nodes with 4\u00d7H100 80GB; 16 nodes with 4\u00d7H100-NVL 96GB; 8 nodes with 8\u00d7L40S 48GB."
    },
    {
      "url": "https://jhpce.jhu.edu/gpu/gpu-info/",
      "data_found": "JHPCE GPU nodes (as of Nov 2024): compute-117 has 2\u00d7V100 32GB + 1\u00d7Titan V 11GB; compute-123 has 4\u00d7V100 32GB; compute-126 has 4\u00d7A100 80GB; compute-128 has 4\u00d7A100 80GB; compute-170 has 2\u00d7H100 96GB; compute-171\u2013173 have 12\u00d7L40S 46GB total (4 per node across 3 nodes)."
    },
    {
      "url": "https://www.cs.jhu.edu/about/message-from-dept-head/",
      "data_found": "Student enrollment: 700 undergrad, 300 grad, 200 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 700,
    "grad_cs_count": 300,
    "phd_cs_count": 200,
    "year": "2024-2025",
    "source_url": "https://www.cs.jhu.edu/about/message-from-dept-head/",
    "notes": "On the CS department page 'Message from the Department Head,' the text explicitly references 'The 2024\u20132025 academic year' and states: 'Our community includes more than 700 undergraduate majors, 300 master\u2019s students, 200 PhD students...'. undergrad_cs_count is recorded as 700 as a lower bound because the page says 'more than 700' (not an exact figure). No explicit 'last updated' date was shown on the page."
  },
  "gpu_resources": {
    "h100_sxm_count": 64,
    "h100_pcie_count": 66,
    "h200_count": 0,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 168,
    "a100_40gb_count": 72,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 108,
    "v100_count": 6,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [],
    "source_url": "https://docs.arch.jhu.edu/en/latest/1_Clusters/DSAI/1_Resources/Hardware.html",
    "notes": "Clusters included: ARCH Rockfish + ARCH DSAI + JHPCE.\n\nRockfish totals:\n- A100 40GB: 18 nodes \u00d7 4 = 72\n- A100 80GB: 10 nodes \u00d7 4 = 40\n- L40S: 4 nodes \u00d7 8 = 32\n\nDSAI totals:\n- A100 80GB: 15 nodes \u00d7 8 = 120\n- H100 80GB: 16 nodes \u00d7 4 = 64\n- H100-NVL 96GB: 16 nodes \u00d7 4 = 64\n- L40S: 8 nodes \u00d7 8 = 64\n\nJHPCE totals:\n- V100: (compute-117: 2) + (compute-123: 4) = 6\n- A100 80GB: (compute-126: 4) + (compute-128: 4) = 8\n- H100 96GB: (compute-170: 2) = 2\n- L40S: (compute-171\u2013173: 12) = 12\n\nGrand totals:\n- A100 80GB: 40 + 120 + 8 = 168\n- A100 40GB: 72\n- V100: 6\n- L40S: 32 + 64 + 12 = 108\n- H100 SXM vs PCIe split assumption: counted DSAI 'H100 80GB' (64) as SXM; counted DSAI 'H100-NVL 96GB' (64) and JHPCE 'H100 96GB' (2) as PCIe-form-factor H100 (total PCIe=66). If DSAI H100 80GB nodes are actually PCIe cards, then h100_sxm_count would be 0 and h100_pcie_count would be 130.\n\nNon-key GPUs encountered: JHPCE lists 1\u00d7NVIDIA Titan V (11GB); not included in output keys."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}