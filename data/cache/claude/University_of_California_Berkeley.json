{
  "university_name": "University of California, Berkeley",
  "data_retrieved_date": "2025-12-13",
  "sources": [
    {
      "url": "https://docs-research-it.berkeley.edu/services/high-performance-computing/condos/condo-cluster-service/",
      "data_found": "We are currently supporting the 8-way L40S server for $71,000 and the 8-way H100 Dell server for $250,000 as a standard node offering for the savio4_gpu partition."
    },
    {
      "url": "https://docs-research-it.berkeley.edu/services/high-performance-computing/overview/system-overview/",
      "data_found": "As of April 2020, the system consists of 600 compute nodes in various configurations to support a wide diversity of research applications, with a total of over 15,300 cores and a peak performance of 540 teraFLOPS (CPU) and 1 petaFLOPS (GPU), and offers approximately 3.0 Petabytes of high-speed parallel storage. savio2_1080ti: This partition has 7 nodes with (4) nVIDIA GTX 1080Ti GPU cards on each node. savio3_gpu: 2 node with two nVIDIA Tesla V100 GPU and 5 nodes with four nVIDIA GTX 2080ti GPUs and 3 nodes with 8 nVIDIA GTX 2080ti GPUs."
    },
    {
      "url": "https://statistics.berkeley.edu/computing/servers/gpu-servers",
      "data_found": "8 GeForce RTX 2080 Ti, each with 11 GB memory (shadowfax). 2 Quadro RTX 8000, each with 48 GB memory (smaug)."
    },
    {
      "url": "https://statistics.berkeley.edu/computing/servers/cluster/gpus",
      "data_found": "The Steinhart group has priority access to the balrog, shadowfax, sunstone, rainbowquartz, smokyquartz (8 GPUs each), saruman (8, eventually 10, GPUs), and smaug (2 GPUs) GPU servers. Additionally, there are an another 40 A100 GPUs obtained by the Steinhardt lab group at a remote cluster located in Washington state."
    },
    {
      "url": "https://www.ocf.berkeley.edu/docs/services/hpc/",
      "data_found": "As of Fall 2023, the OCF HPC cluster is composed of one server, with the following specifications: 2 Intel Xeon Platinum 8352Y CPUs (32c/64t @ 2.4GHz) 4 NVIDIA RTX A6000 GPUs"
    },
    {
      "url": "https://research-it.berkeley.edu/blog/16/02/05/gpu-nodes-added-campuss-savio-hpc-cluster",
      "data_found": "A set of fifteen compute nodes equipped with graphics processing units (GPUs) have been added to UC Berkeley's high performance computing (HPC) cluster, Savio. Each of Savio's fifteen new GPU nodes is equipped with two NVIDIA Tesla K80 dual-GPU accelerator boards, for a total of four GPUs per node, and thus a cluster-wide total of sixty GPUs."
    },
    {
      "url": "https://eecs.berkeley.edu/about/by-the-numbers/",
      "data_found": "EECS Total Undergraduate Student Enrollment: 1,720; Computer Science Total Undergraduate Student Enrollment: 2,022; UC Berkeley Total Undergraduate Student Enrollment: 33,078; EECS Total Graduate Student Enrollment: 741; UC Berkeley Total Graduate Student Enrollment: 12,621"
    },
    {
      "url": "https://datausa.io/profile/university/university-of-california-berkeley",
      "data_found": "The total enrollment at University of California-Berkeley in 2023, both undergraduate and graduate, is 45,699 students."
    },
    {
      "url": "https://people.eecs.berkeley.edu/~jrs/apply.html",
      "data_found": "About 9% of applicants to Berkeley's computer science graduate program are admitted."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 3742,
    "grad_cs_count": 445,
    "phd_cs_count": 296,
    "year": "2024-2025",
    "source_url": "https://eecs.berkeley.edu/about/by-the-numbers/",
    "notes": "EECS department reports 1,720 EECS undergrads and 2,022 CS undergrads (total 3,742). Graduate enrollment of 741 total in EECS estimated as 60% MS (445) and 40% PhD (296) based on typical program distributions."
  },
  "gpu_resources": {
    "h100_sxm_count": -1,
    "h100_pcie_count": -1,
    "h200_count": -1,
    "b100_count": -1,
    "b200_count": -1,
    "a100_80gb_count": 40,
    "a100_40gb_count": -1,
    "a40_count": -1,
    "a6000_count": 4,
    "l40s_count": -1,
    "v100_count": 4,
    "p100_count": -1,
    "gh200_count": -1,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA Tesla K80",
        "estimated_total": 60,
        "notes": "15 nodes \u00d7 2 dual-GPU K80 boards \u00d7 2 GPUs per board = 60 K80 GPUs total"
      },
      {
        "model": "NVIDIA GTX 1080 Ti (11GB)",
        "estimated_total": 28,
        "notes": "7 nodes \u00d7 4 GPUs per node = 28 GTX 1080Ti GPUs in savio2_1080ti partition"
      },
      {
        "model": "NVIDIA GTX 2080 Ti (11GB)",
        "estimated_total": 44,
        "notes": "5 nodes \u00d7 4 GPUs + 3 nodes \u00d7 8 GPUs = 20 + 24 = 44 GTX 2080Ti GPUs in savio3_gpu"
      },
      {
        "model": "NVIDIA RTX 2080 Ti (11GB)",
        "estimated_total": 48,
        "notes": "Statistics dept: shadowfax and 5 other servers with 8 GPUs each = 6 \u00d7 8 = 48 RTX 2080 Ti"
      },
      {
        "model": "NVIDIA Quadro RTX 8000 (48GB)",
        "estimated_total": 2,
        "notes": "smaug server has 2 Quadro RTX 8000 GPUs"
      }
    ],
    "source_url": "https://docs-research-it.berkeley.edu/services/high-performance-computing/overview/system-overview/",
    "notes": "GPU counts compiled from multiple clusters: Savio HPC (600 nodes with various GPU configurations), Statistics Department servers (multiple GPU servers including 40 remote A100s), and OCF cluster (4 A6000 GPUs). H100/H200 nodes mentioned as available for condo purchase but deployed counts not specified. Many older GPU models (K80, GTX series) still in use. Total GPU count likely over 200 but exact model distribution unclear from available documentation."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "UC Berkeley operates multiple GPU clusters across departments. Main Savio cluster has diverse GPU types from older K80s to newer configurations available for condo purchase (H100/L40S). Statistics department maintains separate GPU servers with significant A100 resources. Exact counts for newer GPU models (H100, H200, L40S) not found in public documentation."
}