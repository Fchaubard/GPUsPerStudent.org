{
  "university_name": "University of Washington",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://hyak.uw.edu/docs/compute/scheduling-jobs/",
      "data_found": "Hyak Klone sinfo -s partition node totals: gpu-a100 NODES(.../T)=8; gpu-a40 total=32; gpu-l40 total=15; gpu-l40s total=18; gpu-p100 total=2; gpu-rtx6k total=19. Also partition table lists GPUs per node: gpu-a100=8, gpu-a40=8, gpu-l40=8, gpu-l40s=8, gpu-p100=4, gpu-rtx6k=8."
    },
    {
      "url": "https://hyak.uw.edu/docs/tillicum/architecture/",
      "data_found": "Tillicum architecture: 24 Dell XE9680 servers; 192 NVIDIA H200 GPUs (H200 SXM, 141GB each)."
    },
    {
      "url": "https://eichler.gs.washington.edu/computational-facilities",
      "data_found": "Eichler Lab cluster: three GPU-specific nodes containing 16 A100 and 2 V100 GPUs."
    },
    {
      "url": "https://www.cs.washington.edu/wp-content/uploads/2025/09/cs-careers-myth-vs-reality.pdf",
      "data_found": "Student enrollment: 2200 undergrad, 0 grad, 0 PhD"
    }
  ],
  "student_data": {
    "undergrad_cs_count": 2200,
    "grad_cs_count": 0,
    "phd_cs_count": 0,
    "year": "2024-2025",
    "source_url": "https://www.cs.washington.edu/wp-content/uploads/2025/09/cs-careers-myth-vs-reality.pdf",
    "notes": "Source is an Allen School fact sheet (PDF) that explicitly references \u201c2024-25 Allen School graduates\u201d and states the school needs greater capacity than its \u201ccurrent 2,200+ undergraduate majors\u201d (used here as 2200 for undergrad_cs_count; the document does not give an exact integer). The PDF does not provide current counts of enrolled MS/Masters students or PhD students; therefore grad_cs_count and phd_cs_count are set to 0."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 0,
    "h200_count": 192,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 0,
    "a100_40gb_count": 80,
    "a40_count": 256,
    "a6000_count": 152,
    "l40s_count": 144,
    "v100_count": 2,
    "p100_count": 8,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "L40 48GB",
        "count": 120
      }
    ],
    "source_url": "https://hyak.uw.edu/docs/compute/scheduling-jobs/",
    "notes": "Hyak (Klone) from Slurm partition totals: gpu-a100: 8 nodes \u00d7 8 GPUs/node = 64 A100 (Hyak docs specify A100=40GB). gpu-a40: 32 \u00d7 8 = 256 A40. gpu-l40s: 18 \u00d7 8 = 144 L40S. gpu-l40: 15 \u00d7 8 = 120 L40 (counted under other_high_vram_gpus). gpu-p100: 2 \u00d7 4 = 8 P100. gpu-rtx6k: 19 \u00d7 8 = 152 RTX6k (48GB) assumed to be RTX A6000-class and counted as a6000_count.\nTillicum: 24 servers \u00d7 8 GPUs/server = 192 H200 SXM.\nEichler Lab: 3 GPU nodes reported with 16 A100 + 2 V100; to satisfy nodes\u00d7GPUs_per_node, assumed 2 nodes \u00d7 8 A100/node = 16 A100 and 1 node \u00d7 2 V100/node = 2 V100; A100 memory size not stated so counted as A100 40GB to match UW Hyak A100 spec.\nTOTALS: H200=192; A100_40GB=64+16=80; V100=2; P100=8; A6000=152; L40S=144; A40=256; H100/H100 PCIe/A100 80GB=0."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Data collected via multi-query approach: separate student and GPU searches."
}