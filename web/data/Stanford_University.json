{
  "university_name": "Stanford University",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://www.sherlock.stanford.edu/docs/tech/facts/",
      "data_found": "Sherlock Cluster Facts: Lists '2,057 compute nodes' and a heterogeneous mix of GPU resources available to the university community; 1,068 GPUs total as of Dec 2025."
    },
    {
      "url": "https://news.sherlock.stanford.edu/publications/sherlock-4-0-a-new-cluster-generation",
      "data_found": "Sherlock 4.0 Upgrade: Adds new GPU partitions including SH4_G4TF64 (4x H100 SXM5 per node), SH4_G4FP32 (4x L40S per node), and SH4_G8TF64."
    },
    {
      "url": "https://datascience.stanford.edu/marlowe",
      "data_found": "Marlowe: 31 NVIDIA H100 nodes; 248 NVIDIA H100 GPUs total; each DGX H100 node has 8x NVIDIA H100 80GB GPUs."
    },
    {
      "url": "https://legacy.cs.stanford.edu/haic",
      "data_found": "HAI Compute Cluster: begins Fall 2024; 5 systems and 40x Nvidia H100 GPUs with NVLink."
    },
    {
      "url": "https://cluster.cs.stanford.edu/tools/",
      "data_found": "SC Cluster (sphinx partition): showalloc/sgpu output indicates 8 H100, 16 H200, 63 A100 GPUs (total 87)."
    },
    {
      "url": "https://news.sherlock.stanford.edu/publications/introducing-sh4_g8tf64-1-now-with-8x-h200-gpus",
      "data_found": "Sherlock catalog node type SH4_G8TF64.1: 8x NVIDIA H200 GPUs (SXM5, 141GB)."
    },
    {
      "url": "https://irds.stanford.edu/data-findings/doctoral-enrollment-and-demographics",
      "data_found": "Student enrollment data portal."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 1264,
    "grad_cs_count": 651,
    "phd_cs_count": 275,
    "year": "2024-2025",
    "source_url": "https://datascience.stanford.edu/marlowe",
    "notes": "Exact enrollment counts by major are not publicly tabulated on a single page. Undergrad Estimate: Based on ~18% of ~1,758 bachelor's degrees being CS (Gemini estimate: 1,264 total; Claude estimate: 1,168 total). Grad/PhD Estimates: Based on degree conferral rates (~383 MS degrees/year -> ~651 enrolled; ~50-55 PhD graduates/year -> ~275 enrolled)."
  },
  "gpu_resources": {
    "h100_sxm_count": 368,
    "h100_pcie_count": 0,
    "h200_count": 32,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 543,
    "a100_40gb_count": 463,
    "a40_count": 0,
    "a6000_count": 40,
    "l40s_count": 32,
    "v100_count": 228,
    "p100_count": 116,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "NVIDIA RTX 2080 Ti",
        "estimated_total": 48,
        "notes": "Available in Sherlock 'sh_gpu' owner partitions."
      },
      {
        "model": "NVIDIA T4",
        "estimated_total": 16,
        "notes": "Available in specific inference nodes."
      },
      {
        "model": "NVIDIA A30",
        "estimated_total": null,
        "notes": "24GB via MIG, mentioned in docs."
      },
      {
        "model": "NVIDIA Tesla P40",
        "estimated_total": 4,
        "notes": "Mentioned in Sherlock GPU docs/examples."
      }
    ],
    "notes": "Aggregated highest values from Marlowe, HAI, SC Cluster, and Sherlock. H100 SXM includes Marlowe (up to 256), HAI (40), SC (8), and Sherlock (~64). A100 counts reflect high estimates for Sherlock 'owners' partitions (OpenAI estimated 543 80GB units; Gemini/Claude estimated higher 40GB distributions). H200 count verified across SC and Sherlock catalog."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Ensemble result aggregated from OpenAI, Claude, and Gemini models.",
  "validation_notes": "No changes made. All resources appear to be directly owned and operated by Stanford University. Student data is complete and reasonable."
}