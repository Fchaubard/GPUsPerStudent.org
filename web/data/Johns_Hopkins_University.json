{
  "university_name": "Johns Hopkins University",
  "data_retrieved_date": "2025-12-15",
  "sources": [
    {
      "url": "https://www.arch.jhu.edu/systems/rockfish",
      "data_found": "Rockfish Cluster Hardware Overview."
    },
    {
      "url": "https://www.cs.jhu.edu/about/",
      "data_found": "CS Department 'About' page generic stats."
    },
    {
      "url": "https://jhpce.jhu.edu/",
      "data_found": "JHPCE cluster info claiming 23-37 Nvidia GPUs."
    },
    {
      "url": "https://jhpce.jhu.edu/aboutus/model/",
      "data_found": "Cluster specs: 84 compute nodes, 37 Nvidia GPUs."
    },
    {
      "url": "https://www.arch.jhu.edu/support/faq/",
      "data_found": "Rockfish FAQ detailing A100 nodes."
    },
    {
      "url": "https://support.access-ci.org/affinity-groups/rockfish",
      "data_found": "ACCESS-CI specs for Rockfish."
    },
    {
      "url": "https://www.hopkinsmedicine.org/som/education-programs/graduate-programs/johns-hopkins-medical-physics-program/facilities",
      "data_found": "Medical Physics cluster specs (Tesla K40m, Quadro, etc)."
    },
    {
      "url": "https://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-as-demand-for-ai-grows",
      "data_found": "APL usage of DGX H100."
    },
    {
      "url": "https://docs.arch.jhu.edu/en/latest/1_Clusters/Rockfish/1_Resources/Hardware.html",
      "data_found": "Detailed Rockfish GPU node breakdown (A100s, L40S)."
    },
    {
      "url": "https://docs.arch.jhu.edu/en/latest/1_Clusters/DSAI/1_Resources/Hardware.html",
      "data_found": "DSAI cluster breakdown (High count of H100s and A100s)."
    },
    {
      "url": "https://jhpce.jhu.edu/gpu/gpu-info/",
      "data_found": "JHPCE specific node breakdown (V100, A100, H100, L40S)."
    },
    {
      "url": "https://www.cs.jhu.edu/about/message-from-dept-head/",
      "data_found": "Department Head message with 2024-2025 student counts."
    }
  ],
  "student_data": {
    "undergrad_cs_count": 700,
    "grad_cs_count": 300,
    "phd_cs_count": 200,
    "year": "2024-2025",
    "source_url": "https://www.cs.jhu.edu/about/message-from-dept-head/",
    "notes": "Counts sourced from the Department Head's message for the 2024-2025 academic year. The source states 'more than 700 undergraduate majors, 300 master\u2019s students, 200 PhD students'. These figures are higher and more recent than those found in older annual reports or College Factual estimates."
  },
  "gpu_resources": {
    "h100_sxm_count": 0,
    "h100_pcie_count": 0,
    "h200_count": 0,
    "b100_count": 0,
    "b200_count": 0,
    "a100_80gb_count": 0,
    "a100_40gb_count": 0,
    "a40_count": 0,
    "a6000_count": 0,
    "l40s_count": 0,
    "v100_count": 0,
    "p100_count": 0,
    "gh200_count": 0,
    "other_high_vram_gpus": [
      {
        "model": "Tesla K40m",
        "estimated_total": 0,
        "notes": "Medical Physics cluster (12 GB RAM)"
      },
      {
        "model": "Quadro P5000",
        "estimated_total": 0,
        "notes": "Medical Physics cluster"
      },
      {
        "model": "Tesla C1060",
        "estimated_total": 0,
        "notes": "Medical Physics cluster (4 GB RAM)"
      },
      {
        "model": "NVIDIA RTX / V100 (CLSP Grid)",
        "estimated_total": null,
        "notes": "The Center for Language and Speech Processing (CLSP) operates 'The Grid' with over 60 GPU nodes, exact breakdown unknown."
      }
    ],
    "notes": "Aggregated counts from Rockfish, DSAI, and JHPCE clusters. \n1. H100 Counts: DSAI provides 16 nodes with 4x H100 80GB (64 SXM assumed) and 16 nodes with 4x H100-NVL 96GB (64 PCIe/NVL). JHPCE adds 2 H100 96GB (Total PCIe: 66). \n2. A100 Counts: High 80GB count (168) primarily from DSAI (15x8=120) plus Rockfish (40) and JHPCE (8). High 40GB count (76) from Rockfish (19 nodes x 4 GPUs). \n3. L40S: Significant presence in DSAI and Rockfish clusters (Total 108). \n4. V100: 8 found in JHPCE cluster."
  },
  "compute_credits": {
    "total_annual_value_usd": 0.0,
    "description": "Not searched in this query."
  },
  "analysis_notes": "Ensemble result aggregated from OpenAI, Claude, and Gemini models. Student data reflects the most current 2024-2025 academic year figures. GPU data combines the distinct 'DSAI' and 'Rockfish' clusters documented by ARCH, alongside the 'JHPCE' cluster and Medical Physics resources. Discrepancy in A100 40GB counts (76 vs 72) resolved by using the higher node count reported by Gemini.",
  "validation_notes": "GPU CHANGES: All GPU counts set to 0. Rockfish and DSAI are part of the ACCESS-CI program (previously XSEDE), making them shared national resources. JHPCE is also likely a shared resource. The medical physics cluster is also removed due to lack of clear ownership. It is better to undercount than overcount."
}